package integration

import (
	"context"
	"encoding/json"
	"os"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"go.uber.org/zap"

	"dev.helix.agent/internal/messaging"
	"dev.helix.agent/internal/messaging/kafka"
)

// skipIfNoKafka skips the test if Kafka infrastructure is not available
func skipIfNoKafka(t *testing.T) *kafka.Broker {
	t.Helper()

	// Skip in short mode - these tests require external Kafka infrastructure
	if testing.Short() {
		t.Skip("Skipping Kafka integration test in short mode")
	}

	// Skip if KAFKA_ENABLED env var is explicitly set to false
	if os.Getenv("KAFKA_ENABLED") == "false" {
		t.Skip("Skipping Kafka integration test - KAFKA_ENABLED=false")
	}

	kafkaHost := os.Getenv("KAFKA_HOST")
	if kafkaHost == "" {
		kafkaHost = "localhost"
	}
	kafkaPort := os.Getenv("KAFKA_PORT")
	if kafkaPort == "" {
		kafkaPort = "9092"
	}

	cfg := &kafka.Config{
		Brokers:            []string{kafkaHost + ":" + kafkaPort},
		ClientID:           "helixagent-test",
		GroupID:            "test-consumers-" + time.Now().Format("20060102150405"),
		AutoOffsetReset:    "earliest",
		EnableAutoCommit:   false,
		AutoCommitInterval: 5 * time.Second,
		BatchSize:          16384,
		BatchTimeout:       10 * time.Millisecond,
		MaxRetries:         3,
		RequiredAcks:       -1,
		DefaultPartitions:  1,
		DefaultReplication: 1,
		FetchMinBytes:      1,
		FetchMaxBytes:      52428800,
		FetchMaxWait:       500 * time.Millisecond,
	}

	logger := zap.NewNop()
	broker := kafka.NewBroker(cfg, logger)

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	if err := broker.Connect(ctx); err != nil {
		t.Skipf("Skipping Kafka integration test - infrastructure not available: %v", err)
	}

	return broker
}

// ensureTopicExists creates a topic if it doesn't exist
// This is needed because Kafka auto-create topics is disabled in test configuration
func ensureTopicExists(t *testing.T, broker *kafka.Broker, topic string) {
	t.Helper()
	ctx, cancel := context.WithTimeout(context.Background(), 15*time.Second)
	defer cancel()

	// Try to create the topic - it will fail silently if it already exists
	config := &kafka.TopicConfig{
		Name:              topic,
		Partitions:        1,
		ReplicationFactor: 1,
	}

	// Retry topic creation up to 3 times with backoff
	var err error
	for i := 0; i < 3; i++ {
		err = broker.CreateTopic(ctx, config)
		if err == nil {
			break
		}
		t.Logf("Note: CreateTopic attempt %d returned: %v", i+1, err)
		time.Sleep(time.Duration(i+1) * time.Second)
	}

	// Give Kafka time to propagate the topic metadata
	// This is important when auto-create is disabled
	time.Sleep(2 * time.Second)
}

// TestKafka_Connect_Success tests successful connection to Kafka
func TestKafka_Connect_Success(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	assert.True(t, broker.IsConnected())
}

// TestKafka_Connect_AlreadyConnected tests connecting when already connected
func TestKafka_Connect_AlreadyConnected(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	// Connect again should be idempotent
	ctx := context.Background()
	err := broker.Connect(ctx)
	assert.NoError(t, err)
	assert.True(t, broker.IsConnected())
}

// TestKafka_HealthCheck_Connected tests health check when connected
func TestKafka_HealthCheck_Connected(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx := context.Background()
	err := broker.HealthCheck(ctx)
	assert.NoError(t, err)
}

// TestKafka_BasicPubSub tests basic publish and subscribe
func TestKafka_BasicPubSub(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.basic.pubsub." + time.Now().Format("20060102150405")

	// Ensure topic exists first (required when auto-create is disabled)
	ensureTopicExists(t, broker, topic)

	// Subscribe first
	received := make(chan *messaging.Message, 1)
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received <- msg
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)
	defer sub.Unsubscribe()

	// Wait for subscription to be ready
	time.Sleep(1 * time.Second)

	// Publish message
	msg := &messaging.Message{
		ID:        "test-msg-" + time.Now().Format("20060102150405"),
		Type:      "test.event",
		Payload:   []byte(`{"key":"value","test":true}`),
		Timestamp: time.Now(),
		Headers:   map[string]string{"x-test": "value"},
	}

	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	// Wait for message
	select {
	case m := <-received:
		assert.Equal(t, msg.ID, m.ID)
		assert.Equal(t, "test.event", m.Type)
		assert.Contains(t, string(m.Payload), "value")
	case <-time.After(15 * time.Second):
		t.Fatal("timeout waiting for message")
	}
}

// TestKafka_PublishWithHeaders tests publishing with custom headers
func TestKafka_PublishWithHeaders(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.headers." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Subscribe
	received := make(chan *messaging.Message, 1)
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received <- msg
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)
	defer sub.Unsubscribe()

	time.Sleep(1 * time.Second)

	// Publish message with headers
	msg := &messaging.Message{
		ID:        "test-headers-" + time.Now().Format("20060102150405"),
		Type:      "test.headers.event",
		Payload:   []byte(`{"data":"test"}`),
		Timestamp: time.Now(),
		Headers: map[string]string{
			"x-custom-1":    "value1",
			"x-custom-2":    "value2",
			"x-correlation": "corr-123",
		},
		TraceID: "trace-abc-123",
	}

	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	select {
	case m := <-received:
		assert.Equal(t, msg.ID, m.ID)
		assert.Equal(t, msg.Type, m.Type)
	case <-time.After(15 * time.Second):
		t.Fatal("timeout waiting for message")
	}
}

// TestKafka_PublishWithKey tests publishing with custom key
func TestKafka_PublishWithKey(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.keys." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Subscribe
	received := make(chan *messaging.Message, 1)
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received <- msg
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)
	defer sub.Unsubscribe()

	time.Sleep(1 * time.Second)

	// Publish with custom key for partitioning
	msg := &messaging.Message{
		ID:        "test-key-" + time.Now().Format("20060102150405"),
		Type:      "test.keyed.event",
		Payload:   []byte(`{"partition_key":"user-123"}`),
		Timestamp: time.Now(),
	}

	err = broker.Publish(ctx, topic, msg, messaging.WithMessageKey([]byte("user-123")))
	require.NoError(t, err)

	select {
	case m := <-received:
		assert.Equal(t, msg.ID, m.ID)
	case <-time.After(15 * time.Second):
		t.Fatal("timeout waiting for message")
	}
}

// TestKafka_BatchPublish tests batch publishing
func TestKafka_BatchPublish(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	topic := "test.batch." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Subscribe
	var receivedCount atomic.Int64
	handler := func(ctx context.Context, msg *messaging.Message) error {
		receivedCount.Add(1)
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)
	defer sub.Unsubscribe()

	time.Sleep(1 * time.Second)

	// Publish batch
	batchSize := 10
	messages := make([]*messaging.Message, batchSize)
	for i := 0; i < batchSize; i++ {
		messages[i] = &messaging.Message{
			ID:        "batch-msg-" + string(rune('A'+i)),
			Type:      "test.batch.event",
			Payload:   []byte(`{"batch_index":` + string(rune('0'+i)) + `}`),
			Timestamp: time.Now(),
			Headers:   map[string]string{"batch-index": string(rune('0' + i))},
		}
	}

	err = broker.PublishBatch(ctx, topic, messages)
	require.NoError(t, err)

	// Wait for messages
	deadline := time.After(30 * time.Second)
	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case <-deadline:
			t.Fatalf("timeout waiting for batch messages, received: %d", receivedCount.Load())
		case <-ticker.C:
			if receivedCount.Load() >= int64(batchSize) {
				return // Success
			}
		}
	}
}

// TestKafka_SubscribeWithGroupID tests subscribing with custom group ID
func TestKafka_SubscribeWithGroupID(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.groupid." + time.Now().Format("20060102150405")
	customGroup := "custom-group-" + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Subscribe with custom group ID
	received := make(chan *messaging.Message, 1)
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received <- msg
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler, messaging.WithGroupID(customGroup))
	require.NoError(t, err)
	defer sub.Unsubscribe()

	time.Sleep(1 * time.Second)

	// Publish
	msg := &messaging.Message{
		ID:        "test-groupid-" + time.Now().Format("20060102150405"),
		Type:      "test.groupid.event",
		Payload:   []byte(`{"group":"test"}`),
		Timestamp: time.Now(),
	}

	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	select {
	case m := <-received:
		assert.Equal(t, msg.ID, m.ID)
	case <-time.After(15 * time.Second):
		t.Fatal("timeout waiting for message")
	}
}

// TestKafka_SubscribeWithOffsetReset tests subscribing with offset reset option
func TestKafka_SubscribeWithOffsetReset(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.offset." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// First publish a message before subscribing
	msg := &messaging.Message{
		ID:        "test-offset-" + time.Now().Format("20060102150405"),
		Type:      "test.offset.event",
		Payload:   []byte(`{"offset":"test"}`),
		Timestamp: time.Now(),
	}

	err := broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	time.Sleep(500 * time.Millisecond)

	// Subscribe with earliest offset reset
	received := make(chan *messaging.Message, 1)
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received <- msg
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler, messaging.WithOffsetReset(messaging.OffsetResetEarliest))
	require.NoError(t, err)
	defer sub.Unsubscribe()

	// Should receive the message that was published before subscribing
	select {
	case m := <-received:
		assert.Equal(t, msg.ID, m.ID)
	case <-time.After(15 * time.Second):
		t.Fatal("timeout waiting for message with earliest offset reset")
	}
}

// TestKafka_Close_WithActiveSubscriptions tests closing with active subscriptions
func TestKafka_Close_WithActiveSubscriptions(t *testing.T) {
	broker := skipIfNoKafka(t)

	ctx := context.Background()
	topic := "test.close." + time.Now().Format("20060102150405")

	// Ensure topics exist first
	ensureTopicExists(t, broker, topic)
	ensureTopicExists(t, broker, topic+".2")

	// Create multiple subscriptions
	handler := func(ctx context.Context, msg *messaging.Message) error {
		return nil
	}

	sub1, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)

	sub2, err := broker.Subscribe(ctx, topic+".2", handler)
	require.NoError(t, err)

	// Close should clean up all subscriptions
	err = broker.Close(ctx)
	assert.NoError(t, err)
	assert.False(t, broker.IsConnected())

	// Subscriptions should be inactive
	assert.False(t, sub1.IsActive())
	assert.False(t, sub2.IsActive())
}

// TestKafka_Unsubscribe tests unsubscribing
func TestKafka_Unsubscribe(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx := context.Background()
	topic := "test.unsub." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	var received atomic.Int64
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received.Add(1)
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)

	time.Sleep(1 * time.Second)

	// Publish and verify receipt
	msg := &messaging.Message{
		ID:        "test-unsub-" + time.Now().Format("20060102150405"),
		Type:      "test",
		Payload:   []byte(`{}`),
		Timestamp: time.Now(),
	}

	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	time.Sleep(2 * time.Second)
	beforeUnsub := received.Load()

	// Unsubscribe
	err = sub.Unsubscribe()
	require.NoError(t, err)
	assert.False(t, sub.IsActive())

	// Publish again - should not be received
	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	time.Sleep(2 * time.Second)
	afterUnsub := received.Load()

	// Count should be the same (no new messages received)
	assert.Equal(t, beforeUnsub, afterUnsub)
}

// TestKafka_Metrics tests metrics collection
func TestKafka_Metrics(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx := context.Background()
	topic := "test.metrics." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Publish some messages
	for i := 0; i < 5; i++ {
		msg := &messaging.Message{
			ID:        "metrics-" + string(rune('0'+i)),
			Type:      "test",
			Payload:   []byte(`{"test":true}`),
			Timestamp: time.Now(),
		}
		err := broker.Publish(ctx, topic, msg)
		require.NoError(t, err)
	}

	metrics := broker.GetMetrics()
	assert.NotNil(t, metrics)
	assert.GreaterOrEqual(t, metrics.MessagesPublished.Load(), int64(5))
}

// TestKafka_HandlerError tests handler returning error
func TestKafka_HandlerError(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.error." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	var processCount atomic.Int64
	handler := func(ctx context.Context, msg *messaging.Message) error {
		processCount.Add(1)
		// Return error - message should not be committed
		return assert.AnError
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)
	defer sub.Unsubscribe()

	time.Sleep(1 * time.Second)

	// Publish message
	msg := &messaging.Message{
		ID:        "test-error-" + time.Now().Format("20060102150405"),
		Type:      "test.error.event",
		Payload:   []byte(`{"will_error":true}`),
		Timestamp: time.Now(),
	}

	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	// Wait a bit for the handler to be called
	time.Sleep(5 * time.Second)

	// Handler should have been called at least once
	assert.GreaterOrEqual(t, processCount.Load(), int64(1))
}

// TestKafka_LargeMessage tests publishing large messages
func TestKafka_LargeMessage(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topic := "test.large." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Create a large payload (500KB)
	largeData := make([]byte, 500*1024)
	for i := range largeData {
		largeData[i] = byte(i % 256)
	}

	payload, _ := json.Marshal(map[string]interface{}{
		"data": largeData,
	})

	// Subscribe
	received := make(chan *messaging.Message, 1)
	handler := func(ctx context.Context, msg *messaging.Message) error {
		received <- msg
		return nil
	}

	sub, err := broker.Subscribe(ctx, topic, handler)
	require.NoError(t, err)
	defer sub.Unsubscribe()

	time.Sleep(1 * time.Second)

	// Publish large message
	msg := &messaging.Message{
		ID:        "test-large-" + time.Now().Format("20060102150405"),
		Type:      "test.large.event",
		Payload:   payload,
		Timestamp: time.Now(),
	}

	err = broker.Publish(ctx, topic, msg)
	require.NoError(t, err)

	select {
	case m := <-received:
		assert.Equal(t, msg.ID, m.ID)
		assert.True(t, len(m.Payload) > 500*1024) // Should be larger due to JSON encoding
	case <-time.After(15 * time.Second):
		t.Fatal("timeout waiting for large message")
	}
}

// TestKafka_ConcurrentPublish tests concurrent publishing
func TestKafka_ConcurrentPublish(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx := context.Background()
	topic := "test.concurrent." + time.Now().Format("20060102150405")

	// Ensure topic exists first
	ensureTopicExists(t, broker, topic)

	// Concurrent publishers
	numPublishers := 5
	messagesPerPublisher := 10
	done := make(chan bool, numPublishers)

	for i := 0; i < numPublishers; i++ {
		go func(publisherID int) {
			for j := 0; j < messagesPerPublisher; j++ {
				msg := &messaging.Message{
					ID:        "concurrent-" + string(rune('0'+publisherID)) + "-" + string(rune('0'+j)),
					Type:      "test.concurrent.event",
					Payload:   []byte(`{"publisher":` + string(rune('0'+publisherID)) + `}`),
					Timestamp: time.Now(),
				}
				broker.Publish(ctx, topic, msg)
			}
			done <- true
		}(i)
	}

	// Wait for all publishers
	for i := 0; i < numPublishers; i++ {
		<-done
	}

	metrics := broker.GetMetrics()
	expected := int64(numPublishers * messagesPerPublisher)
	assert.GreaterOrEqual(t, metrics.MessagesPublished.Load(), expected)
}

// TestKafka_CreateTopic tests topic creation
func TestKafka_CreateTopic(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	topicName := "test.create." + time.Now().Format("20060102150405")
	topicConfig := &kafka.TopicConfig{
		Name:              topicName,
		Partitions:        3,
		ReplicationFactor: 1,
	}

	err := broker.CreateTopic(ctx, topicConfig)
	// Topic creation may fail if already exists, which is OK
	if err != nil {
		t.Logf("Topic creation note: %v", err)
	}

	// Verify we can publish to the topic
	msg := &messaging.Message{
		ID:        "test-create-" + time.Now().Format("20060102150405"),
		Type:      "test",
		Payload:   []byte(`{}`),
		Timestamp: time.Now(),
	}
	err = broker.Publish(ctx, topicName, msg)
	assert.NoError(t, err)
}

// TestKafka_GetTopicMetadata tests getting topic metadata
func TestKafka_GetTopicMetadata(t *testing.T) {
	broker := skipIfNoKafka(t)
	defer broker.Close(context.Background())

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	// First create a topic
	topicName := "test.metadata." + time.Now().Format("20060102150405")
	topicConfig := &kafka.TopicConfig{
		Name:              topicName,
		Partitions:        2,
		ReplicationFactor: 1,
	}
	_ = broker.CreateTopic(ctx, topicConfig)

	// Publish to ensure topic exists
	msg := &messaging.Message{
		ID:        "test-metadata-" + time.Now().Format("20060102150405"),
		Type:      "test",
		Payload:   []byte(`{}`),
		Timestamp: time.Now(),
	}
	err := broker.Publish(ctx, topicName, msg)
	require.NoError(t, err)

	time.Sleep(1 * time.Second)

	// Get metadata
	metadata, err := broker.GetTopicMetadata(ctx, topicName)
	if err != nil {
		t.Skipf("Could not get metadata: %v", err)
	}

	assert.Equal(t, topicName, metadata.Name)
	assert.NotEmpty(t, metadata.Partitions)
}
