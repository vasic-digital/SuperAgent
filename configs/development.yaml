# HelixAgent Development Configuration
# This configuration is optimized for development environments

server:
  port: 7061
  host: "0.0.0.0"
  environment: "development"
  log_level: "debug"
  
  # Development-specific settings
  debug:
    enabled: true
    pprof_enabled: true
    pprof_port: 6060
    verbose_logging: true

database:
  # PostgreSQL configuration
  url: "postgres://helixagent:dev_password@localhost:5432/helixagent_dev?sslmode=disable"
  host: "localhost"
  port: 5432
  user: "helixagent"
  password: "dev_password"
  name: "helixagent_dev"
  sslmode: "disable"
  
  # Connection pool settings (relaxed for development)
  max_open_connections: 10
  max_idle_connections: 2
  connection_max_lifetime: "5m"

redis:
  # Redis configuration
  url: "redis://localhost:6379/0"
  host: "localhost"
  port: 6379
  password: ""
  db: 0

  # Connection pool settings
  pool_size: 5
  min_idle_connections: 1

cognee:
  # Cognee knowledge graph configuration
  # PERMANENTLY DISABLED: Replaced by Mem0 memory system
  # Root cause: RAG_COMPLETION hangs on empty database (>35s timeout)
  enabled: false   # DISABLED - Mem0 is now primary memory provider
  base_url: "http://localhost:8000"
  api_key: ""
  auto_cognify: false
  timeout: "30s"

# Mem0-Style Memory System (PRIMARY MEMORY PROVIDER)
memory:
  # Storage backend
  storage_type: "postgres"  # Use existing PostgreSQL with pgvector
  vectordb_endpoint: "localhost:5432"
  vectordb_api_key: ""
  vectordb_collection: "helixagent_memories"

  # Embedding configuration (using OpenAI)
  embedding_model: "text-embedding-3-small"
  embedding_endpoint: "${OPENAI_API_KEY}"
  embedding_dimension: 1536

  # Memory management settings
  max_memories_per_user: 10000
  memory_ttl: 0  # 0 = no expiration
  enable_graph: true  # Entity graph support
  enable_compression: true

  # LLM for memory extraction (using ensemble)
  extractor_model: "gpt-4"
  extractor_endpoint: "${OPENAI_API_KEY}"
  extractor_api_key: "${OPENAI_API_KEY}"

llm_providers:
  # OpenAI (primary for development)
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4"
    base_url: "https://api.openai.com/v1"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 1.0

  # Anthropic Claude (fallback)
  anthropic:
    enabled: true
    api_key: "${ANTHROPIC_API_KEY}"
    model: "claude-3-sonnet-20240229"
    base_url: "https://api.anthropic.com"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 0.8

  # Google Gemini (highest LLMsVerifier score: 8.6)
  gemini:
    enabled: true
    api_key: "${GEMINI_API_KEY}"
    model: "gemini-1.5-pro"
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 1.0

  # DeepSeek (LLMsVerifier score: 8.0)
  deepseek:
    enabled: true
    api_key: "${DEEPSEEK_API_KEY}"
    model: "deepseek-chat"
    base_url: "https://api.deepseek.com/v1"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 0.9

  # Mistral (LLMsVerifier score: 7.0)
  mistral:
    enabled: true
    api_key: "${MISTRAL_API_KEY}"
    model: "mistral-large-latest"
    base_url: "https://api.mistral.ai/v1"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 0.8

  # Cerebras (LLMsVerifier score: 7.0 - fast inference)
  cerebras:
    enabled: true
    api_key: "${CEREBRAS_API_KEY}"
    model: "llama-3.3-70b"
    base_url: "https://api.cerebras.ai/v1"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 0.7

  # Fireworks (disabled - API key invalid)
  fireworks:
    enabled: false
    api_key: "${FIREWORKS_API_KEY}"
    model: "accounts/fireworks/models/llama-v3p1-70b-instruct"
    base_url: "https://api.fireworks.ai/inference/v1"
    temperature: 0.7
    max_tokens: 2048
    timeout: "120s"
    weight: 0.7

security:
  # JWT configuration (development - less secure)
  jwt:
    secret: "dev_jwt_secret_change_in_production"
    expiration: "24h"
    
  # API Key configuration
  api_key:
    header: "X-API-Key"
    length: 16  # Shorter for development
    
  # Rate limiting (relaxed for development)
  rate_limit:
    requests_per_minute: 120
    burst: 20
    
  # CORS (permissive for development)
  cors:
    allowed_origins:
      - "http://localhost:3000"
      - "http://localhost:7061"
      - "http://127.0.0.1:3000"
      - "http://127.0.0.1:7061"
    allowed_methods:
      - "GET"
      - "POST"
      - "PUT"
      - "DELETE"
      - "OPTIONS"
      - "PATCH"
    allowed_headers:
      - "Content-Type"
      - "Authorization"
      - "X-API-Key"
      - "X-Requested-With"

monitoring:
  # Prometheus metrics (enabled for development)
  metrics:
    enabled: true
    port: 9090
    path: "/metrics"
    
  # Health checks
  health_check:
    enabled: true
    path: "/health"
    interval: "30s"

cache:
  # Cache TTL settings (shorter for development)
  default_ttl: 1800  # 30 minutes
  user_session_ttl: 900   # 15 minutes
  api_key_ttl: 3600      # 1 hour
  provider_health_ttl: 120 # 2 minutes
  
  # Cache size limits (smaller for development)
  max_memory_size: "50MB"
  max_entries: 5000

ai_debate:
  # AI Debate configuration
  enabled: true
  max_participants: 5
  max_rounds: 3
  consensus_threshold: 0.6

  # Cognee integration - PERMANENTLY DISABLED
  # Replaced by Mem0 memory system
  # Root cause: RAG_COMPLETION hangs on empty database, blocking all requests
  cognee:
    enabled: false   # DISABLED - Mem0 is now primary memory provider
    api_url: "http://localhost:8000"
    dataset_name: "helixagent_dev_debates"
    # Authentication (form-encoded OAuth2 style)
    auth_email: "admin@helixagent.ai"
    auth_password: "HelixAgentPass123"

optimization:
  # LLM Optimization Framework
  enabled: true

  # Semantic cache (GPTCache-inspired)
  semantic_cache:
    enabled: true
    similarity_threshold: 0.85
    max_entries: 10000
    ttl: "24h"
    embedding_model: "text-embedding-3-small"
    eviction_policy: "lru_with_relevance"

  # Structured output generation (Outlines-inspired)
  structured_output:
    enabled: true
    strict_mode: true
    retry_on_failure: true
    max_retries: 3

  # Enhanced streaming
  streaming:
    enabled: true
    buffer_type: "word"  # character, word, sentence, line, paragraph, token
    progress_interval: "100ms"
    rate_limit: 0  # 0 = unlimited

  # SGLang integration (RadixAttention prefix caching)
  sglang:
    enabled: false  # Requires GPU
    endpoint: "http://localhost:30000"
    timeout: "120s"
    fallback_on_unavailable: true

  # LlamaIndex integration (document retrieval)
  llamaindex:
    enabled: false  # Start service manually if needed
    endpoint: "http://localhost:8012"
    timeout: "120s"
    use_cognee_index: true

  # LangChain integration (task decomposition)
  langchain:
    enabled: false  # Start service manually if needed
    endpoint: "http://localhost:8011"
    timeout: "120s"
    default_chain: "react"

  # Guidance integration (CFG/regex constraints)
  guidance:
    enabled: false  # Start service manually if needed
    endpoint: "http://localhost:8013"
    timeout: "120s"
    cache_programs: true

  # LMQL integration (query language)
  lmql:
    enabled: false  # Start service manually if needed
    endpoint: "http://localhost:8014"
    timeout: "120s"
    cache_queries: true

  # Fallback behavior
  fallback:
    on_service_unavailable: "skip"  # skip, error, cache_only
    health_check_interval: "30s"
    retry_unavailable_after: "5m"

plugins:
  # Plugin system (enabled for development)
  enabled: true
  directory: "./plugins"
  auto_load: true
  
  # Plugin security (relaxed for development)
  sandbox:
    enabled: false  # Disabled for easier debugging
    max_memory: "256MB"
    max_cpu_time: "60s"

feature_flags:
  # Feature toggles (all enabled for development)
  ai_debate: true
  cognee_integration: false  # Disabled due to upstream Cognee bug (see COGNEE_BUG.md)
  plugin_system: true
  real_time_monitoring: true
  advanced_analytics: true

logging:
  # Logging configuration (verbose for development)
  format: "text"  # More readable than JSON for development
  output: "stdout"
  level: "debug"
  
  # File logging (optional for development)
  file:
    enabled: false
    path: "./logs/helixagent_dev.log"
    max_size: "100MB"
    max_backups: 3
    max_age: "7d"

performance:
  # Performance settings (optimized for development)
  profiling_enabled: true
  graceful_shutdown_timeout: "15s"
  max_request_size: 5242880  # 5MB
  request_timeout: "180s"
  idle_timeout: "300s"
  read_timeout: "180s"
  write_timeout: "180s"

external_services:
  # External integrations (mostly disabled for development)
  webhook:
    enabled: false
    url: ""
    secret: ""
    
  smtp:
    enabled: false
    host: ""
    port: 587
    username: ""
    password: ""
    from: ""
    
  sentry:
    dsn: ""
    environment: "development"
    
  datadog:
    api_key: ""
    app_key: ""

backup:
  # Backup settings (disabled for development)
  enabled: false
  schedule: "0 3 * * *"  # 3 AM daily
  retention_days: 7
  storage_path: "./backups/dev"

services:
  postgresql:
    host: "localhost"
    port: "5432"
    enabled: true
    required: true
    remote: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_type: "pgx"
    timeout: "10s"
    retry_count: 6
    compose_file: "docker-compose.yml"
    service_name: "postgres"
    profile: "default"
  redis:
    host: "localhost"
    port: "6379"
    enabled: true
    required: true
    remote: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_type: "redis"
    timeout: "5s"
    retry_count: 6
    compose_file: "docker-compose.yml"
    service_name: "redis"
    profile: "default"
  cognee:
    host: "localhost"
    port: "8000"
    enabled: false    # DISABLED - Replaced by Mem0 memory system
    required: false   # NOT REQUIRED - Mem0 is now primary memory provider
    remote: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/"
    health_type: "http"
    timeout: "10s"
    retry_count: 6
    compose_file: "docker-compose.yml"
    service_name: "cognee"
    profile: "default"
  chromadb:
    host: "localhost"
    port: "8100"
    enabled: true
    required: true
    remote: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/api/v2/heartbeat"
    health_type: "http"
    timeout: "10s"
    retry_count: 6
    compose_file: "docker-compose.yml"
    service_name: "chromadb"
    profile: "default"
  prometheus:
    host: "localhost"
    port: "9090"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/-/healthy"
    health_type: "http"
  grafana:
    host: "localhost"
    port: "3000"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/api/health"
    health_type: "http"
  neo4j:
    host: "localhost"
    port: "7474"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_type: "http"
  kafka:
    host: "localhost"
    port: "9092"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_type: "tcp"
  rabbitmq:
    host: "localhost"
    port: "5672"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_type: "tcp"
  qdrant:
    host: "localhost"
    port: "6333"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/healthz"
    health_type: "http"
  weaviate:
    host: "localhost"
    port: "8080"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/v1/.well-known/ready"
    health_type: "http"
  langchain:
    host: "localhost"
    port: "8011"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/health"
    health_type: "http"
  llamaindex:
    host: "localhost"
    port: "8012"
    enabled: false
    required: false
    discovery_enabled: false
    discovery_method: "tcp"
    discovery_timeout: "5s"
    health_path: "/health"
    health_type: "http"

modelsdev:
  # Models.dev integration (disabled by default in development)
  enabled: ${MODELSDEV_ENABLED:-false}
  api_key: "${MODELSDEV_API_KEY}"
  base_url: "https://api.models.dev/v1"
  
  # Data refresh and caching settings
  refresh_interval: "24h"
  cache_ttl: "1h"
  default_batch_size: 100
  max_retries: 3
  auto_refresh: true
  
  # API request settings
  timeout: "30s"
  user_agent: "HelixAgent/1.0 (Development)"
  rate_limit: 10
  
  # Advanced settings
  enable_benchmarks: true
  enable_capabilities: true
  enable_providers: true