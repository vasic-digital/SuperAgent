# Prometheus Alert Rules for HelixAgent Ecosystem
# Monitor critical conditions and trigger alerts for all services

groups:
  # ========================================
  # HELIXAGENT CORE ALERTS
  # ========================================

  - name: helixagent_availability
    interval: 30s
    rules:
      - alert: HelixAgentDown
        expr: up{job="helixagent"} == 0
        for: 1m
        labels:
          severity: critical
          service: helixagent
        annotations:
          summary: "HelixAgent is down"
          description: "HelixAgent has been down for more than 1 minute."

      - alert: HealthCheckFailing
        expr: helixagent_health_status == 0
        for: 2m
        labels:
          severity: critical
          service: helixagent
        annotations:
          summary: "HelixAgent health check failing"
          description: "HelixAgent health check has been failing for more than 2 minutes."

      - alert: HelixAgentHighResponseTime
        expr: helixagent_response_time_ms > 5000
        for: 2m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "HelixAgent high response time"
          description: "HelixAgent response time is above 5 seconds. Current: {{ $value }}ms"

  - name: helixagent_performance
    interval: 30s
    rules:
      - alert: HighLatency
        expr: histogram_quantile(0.99, rate(helixagent_llm_request_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "High LLM request latency"
          description: "99th percentile latency is above 30 seconds for 5 minutes. Current: {{ $value }}s"

      - alert: HighErrorRate
        expr: sum(rate(helixagent_llm_request_errors_total[5m])) / sum(rate(helixagent_llm_request_total[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 10% for 5 minutes. Current: {{ $value | humanizePercentage }}"

      - alert: CriticalErrorRate
        expr: sum(rate(helixagent_llm_request_errors_total[5m])) / sum(rate(helixagent_llm_request_total[5m])) > 0.25
        for: 2m
        labels:
          severity: critical
          service: helixagent
        annotations:
          summary: "Critical error rate detected"
          description: "Error rate is above 25% for 2 minutes. Current: {{ $value | humanizePercentage }}"

  - name: helixagent_providers
    interval: 30s
    rules:
      - alert: ProviderDown
        expr: helixagent_provider_health{status="unhealthy"} == 1
        for: 2m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "LLM Provider unhealthy"
          description: "Provider {{ $labels.provider }} has been unhealthy for more than 2 minutes."

      - alert: AllProvidersDown
        expr: helixagent_providers_healthy == 0
        for: 1m
        labels:
          severity: critical
          service: helixagent
        annotations:
          summary: "All LLM providers are down"
          description: "No healthy LLM providers available for more than 1 minute."

      - alert: LowProviderCount
        expr: helixagent_providers_total < 5
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "Low provider count"
          description: "Less than 5 LLM providers available. Current: {{ $value }}"

      - alert: ProviderHighRateLimit
        expr: sum(rate(helixagent_rate_limit_hits_total[5m])) by (provider) > 10
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "High rate limit hits"
          description: "Provider {{ $labels.provider }} is hitting rate limits frequently."

  - name: helixagent_resources
    interval: 30s
    rules:
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 / 1024 > 4
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 4GB for 5 minutes. Current: {{ $value | humanize }}GB"

      - alert: HighCPUUsage
        expr: rate(process_cpu_seconds_total[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 80% for 5 minutes."

      - alert: TooManyOpenConnections
        expr: helixagent_active_connections > 1000
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "Too many open connections"
          description: "Active connections exceeds 1000 for 5 minutes."

  - name: helixagent_debate
    interval: 30s
    rules:
      - alert: LowDebateConsensus
        expr: avg(helixagent_debate_consensus_score) < 0.5
        for: 10m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "Low debate consensus"
          description: "Average debate consensus is below 50% for 10 minutes."

      - alert: DebateTimeout
        expr: sum(rate(helixagent_debate_timeout_total[5m])) > 5
        for: 5m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "Frequent debate timeouts"
          description: "More than 5 debate timeouts in the last 5 minutes."

  - name: helixagent_cost
    interval: 60s
    rules:
      - alert: HighCostPerHour
        expr: sum(rate(helixagent_llm_cost_total_usd[1h])) * 3600 > 10
        for: 30m
        labels:
          severity: warning
          service: helixagent
        annotations:
          summary: "High LLM cost rate"
          description: "LLM costs are exceeding $10/hour for 30 minutes."

      - alert: TokenBudgetExceeded
        expr: sum(helixagent_token_usage_total) > 1000000
        for: 1h
        labels:
          severity: info
          service: helixagent
        annotations:
          summary: "Token budget threshold reached"
          description: "Total token usage has exceeded 1 million tokens in the last hour."

  # ========================================
  # CHROMADB ALERTS
  # ========================================

  - name: chromadb_availability
    interval: 30s
    rules:
      - alert: ChromaDBDown
        expr: chromadb_up == 0
        for: 1m
        labels:
          severity: critical
          service: chromadb
        annotations:
          summary: "ChromaDB is down"
          description: "ChromaDB vector database has been down for more than 1 minute."

      - alert: ChromaDBHighResponseTime
        expr: chromadb_response_time_ms > 2000
        for: 2m
        labels:
          severity: warning
          service: chromadb
        annotations:
          summary: "ChromaDB high response time"
          description: "ChromaDB response time is above 2 seconds. Current: {{ $value }}ms"

      - alert: ChromaDBCollectionsEmpty
        expr: chromadb_collections_total == 0
        for: 5m
        labels:
          severity: info
          service: chromadb
        annotations:
          summary: "No ChromaDB collections"
          description: "ChromaDB has no collections for more than 5 minutes."

  # ========================================
  # COGNEE ALERTS
  # ========================================

  - name: cognee_availability
    interval: 30s
    rules:
      - alert: CogneeDown
        expr: cognee_up == 0
        for: 1m
        labels:
          severity: critical
          service: cognee
        annotations:
          summary: "Cognee is down"
          description: "Cognee knowledge graph service has been down for more than 1 minute."

      - alert: CogneeHighResponseTime
        expr: cognee_response_time_ms > 3000
        for: 2m
        labels:
          severity: warning
          service: cognee
        annotations:
          summary: "Cognee high response time"
          description: "Cognee response time is above 3 seconds. Current: {{ $value }}ms"

  # ========================================
  # LLMSVERIFIER ALERTS
  # ========================================

  - name: llmsverifier_availability
    interval: 30s
    rules:
      - alert: LLMsVerifierDown
        expr: llmsverifier_up == 0
        for: 2m
        labels:
          severity: warning
          service: llmsverifier
        annotations:
          summary: "LLMsVerifier is down"
          description: "LLMsVerifier service has been down for more than 2 minutes."

      - alert: LLMsVerifierHighResponseTime
        expr: llmsverifier_response_time_ms > 5000
        for: 5m
        labels:
          severity: warning
          service: llmsverifier
        annotations:
          summary: "LLMsVerifier high response time"
          description: "LLMsVerifier response time is above 5 seconds. Current: {{ $value }}ms"

  # ========================================
  # POSTGRESQL ALERTS
  # ========================================

  - name: postgres_availability
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          service: postgres
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 1 minute."

      - alert: PostgreSQLHighConnections
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "High PostgreSQL connections"
          description: "PostgreSQL has more than 80 active connections. Current: {{ $value }}"

      - alert: PostgreSQLDeadlocks
        expr: rate(pg_stat_database_deadlocks[5m]) > 0
        for: 1m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL deadlocks detected"
          description: "PostgreSQL is experiencing deadlocks."

      - alert: PostgreSQLSlowQueries
        expr: pg_stat_activity_max_tx_duration > 300
        for: 5m
        labels:
          severity: warning
          service: postgres
        annotations:
          summary: "PostgreSQL slow queries"
          description: "PostgreSQL has queries running longer than 5 minutes."

  # ========================================
  # REDIS ALERTS
  # ========================================

  - name: redis_availability
    interval: 30s
    rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis is down"
          description: "Redis cache has been down for more than 1 minute."

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis high memory usage"
          description: "Redis memory usage is above 85%. Current: {{ $value | humanizePercentage }}"

      - alert: RedisRejectedConnections
        expr: rate(redis_rejected_connections_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Redis rejecting connections"
          description: "Redis is rejecting connections."

      - alert: RedisKeyEvictions
        expr: rate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "High Redis key eviction rate"
          description: "Redis is evicting more than 100 keys per 5 minutes."

  # ========================================
  # MCP ALERTS
  # ========================================

  - name: mcp_availability
    interval: 30s
    rules:
      - alert: LowMCPServerCount
        expr: helixagent_mcp_servers_total < 10
        for: 5m
        labels:
          severity: warning
          service: mcp
        annotations:
          summary: "Low MCP server count"
          description: "Less than 10 MCP servers available. Current: {{ $value }}"

      - alert: NoMCPServers
        expr: helixagent_mcp_servers_total == 0
        for: 1m
        labels:
          severity: critical
          service: mcp
        annotations:
          summary: "No MCP servers available"
          description: "No MCP servers are available."

  # ========================================
  # INFRASTRUCTURE ALERTS
  # ========================================

  - name: node_availability
    interval: 30s
    rules:
      - alert: HighNodeCPU
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High node CPU usage"
          description: "Node CPU usage is above 80%. Current: {{ $value }}%"

      - alert: HighNodeMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High node memory usage"
          description: "Node memory usage is above 85%. Current: {{ $value }}%"

      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 10
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Low disk space"
          description: "Root filesystem has less than 10% free space. Current: {{ $value }}%"

  # ========================================
  # CONTAINER ALERTS
  # ========================================

  - name: container_availability
    interval: 30s
    rules:
      - alert: ContainerHighCPU
        expr: sum(rate(container_cpu_usage_seconds_total[5m])) by (name) > 0.8
        for: 5m
        labels:
          severity: warning
          service: containers
        annotations:
          summary: "Container high CPU"
          description: "Container {{ $labels.name }} CPU usage is above 80%."

      - alert: ContainerHighMemory
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: containers
        annotations:
          summary: "Container high memory"
          description: "Container {{ $labels.name }} memory usage is above 90%."

      - alert: ContainerRestarting
        expr: increase(container_restart_count[1h]) > 3
        for: 5m
        labels:
          severity: warning
          service: containers
        annotations:
          summary: "Container restarting frequently"
          description: "Container {{ $labels.name }} has restarted more than 3 times in the last hour."
