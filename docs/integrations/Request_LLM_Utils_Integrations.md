GPTCache, Semantic cache for LLM queries that reduces redundant API calls by storing identical or similar responses, https://github.com/zilliztech/GPTCache

LangChain, Framework for orchestrating multi-step LLM calls, tool use, and memory management to break down complex tasks, https://github.com/langchain-ai/langchain

LlamaIndex, Data framework for indexing and retrieving only relevant context from documents to keep prompts focused, https://github.com/run-llama/llama_index

Guidance, Pythonic library for controlling LLM output with constraints like regex and grammar rules to reduce token generation, https://github.com/guidance-ai/guidance

LMQL, Query language for LLMs with constraint-guided decoding and speculative execution for efficient generation, https://github.com/eth-sri/lmql

Outlines, Library for guaranteed structured outputs (JSON, Pydantic) that eliminates retries and reduces token waste, https://github.com/dottxt-ai/outlines

llm-streaming, Tools and examples for implementing real-time token streaming to improve perceived responsiveness, https://github.com/SaihanTaki/llm-streaming

SGLang, High-performance serving framework with client-side libraries leveraging RadixAttention for prefix caching, https://github.com/sgl-project/sglang
