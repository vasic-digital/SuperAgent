// Package models provides extended tests for the embedding model registry.
// This file contains additional comprehensive tests for edge cases, error conditions,
// and thorough coverage of all exported functions and methods.
package models

import (
	"context"
	"encoding/json"
	"errors"
	"net/http"
	"net/http/httptest"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/sirupsen/logrus"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// =============================================================================
// EmbeddingModelConfig Tests
// =============================================================================

func TestEmbeddingModelConfig_JSONSerialization(t *testing.T) {
	tests := []struct {
		name   string
		config EmbeddingModelConfig
	}{
		{
			name: "full config",
			config: EmbeddingModelConfig{
				Name:         "test-model",
				Provider:     "openai",
				ModelID:      "text-embedding-3-small",
				Dimensions:   1536,
				MaxTokens:    8191,
				BatchSize:    100,
				Timeout:      30 * time.Second,
				CacheEnabled: true,
				CacheTTL:     1 * time.Hour,
				BaseURL:      "https://api.openai.com/v1",
				APIKey:       "test-key",
			},
		},
		{
			name: "minimal config",
			config: EmbeddingModelConfig{
				Name:     "minimal",
				Provider: "local",
			},
		},
		{
			name: "ollama config",
			config: EmbeddingModelConfig{
				Name:       "ollama-test",
				Provider:   "ollama",
				ModelID:    "nomic-embed-text",
				Dimensions: 768,
				BaseURL:    "http://localhost:11434",
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			data, err := json.Marshal(tt.config)
			require.NoError(t, err)

			var decoded EmbeddingModelConfig
			err = json.Unmarshal(data, &decoded)
			require.NoError(t, err)

			assert.Equal(t, tt.config.Name, decoded.Name)
			assert.Equal(t, tt.config.Provider, decoded.Provider)
			assert.Equal(t, tt.config.ModelID, decoded.ModelID)
			assert.Equal(t, tt.config.Dimensions, decoded.Dimensions)
		})
	}
}

// =============================================================================
// EmbeddingModelRegistry Advanced Tests
// =============================================================================

func TestEmbeddingModelRegistry_FallbackChainBehavior(t *testing.T) {
	tests := []struct {
		name          string
		fallbackChain []string
		expectedFirst string
	}{
		{
			name:          "empty fallback chain uses default",
			fallbackChain: nil,
			expectedFirst: "openai-3-small",
		},
		{
			name:          "custom fallback chain",
			fallbackChain: []string{"custom1", "custom2"},
			expectedFirst: "custom1",
		},
		{
			name:          "single item fallback chain",
			fallbackChain: []string{"single"},
			expectedFirst: "single",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			registry := NewEmbeddingModelRegistry(RegistryConfig{
				FallbackChain: tt.fallbackChain,
			})

			if len(tt.fallbackChain) == 0 {
				assert.Equal(t, []string{"openai-3-small", "bge-m3", "all-mpnet-base-v2", "local-fallback"}, registry.fallbackChain)
			} else {
				assert.Equal(t, tt.fallbackChain, registry.fallbackChain)
			}
		})
	}
}

func TestEmbeddingModelRegistry_DefaultModelSelection(t *testing.T) {
	tests := []struct {
		name            string
		defaultModel    string
		fallbackChain   []string
		expectedDefault string
	}{
		{
			name:            "explicit default model",
			defaultModel:    "my-default",
			expectedDefault: "my-default",
		},
		{
			name:            "default from fallback chain",
			defaultModel:    "",
			fallbackChain:   []string{"first-model", "second-model"},
			expectedDefault: "first-model",
		},
		{
			name:            "default when no fallback chain",
			defaultModel:    "",
			fallbackChain:   nil,
			expectedDefault: "openai-3-small", // First in default fallback chain
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			registry := NewEmbeddingModelRegistry(RegistryConfig{
				DefaultModel:  tt.defaultModel,
				FallbackChain: tt.fallbackChain,
			})

			assert.Equal(t, tt.expectedDefault, registry.defaultModel)
		})
	}
}

func TestEmbeddingModelRegistry_RegisterMultipleModels(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	models := make([]*MockEmbeddingModel, 10)
	for i := 0; i < 10; i++ {
		name := "model-" + string(rune('a'+i))
		models[i] = NewMockEmbeddingModel(name, 768)
		// Also add the config so it shows up in List()
		registry.configs[name] = EmbeddingModelConfig{
			Name:       name,
			Provider:   "mock",
			Dimensions: 768,
		}
		err := registry.Register(name, models[i])
		require.NoError(t, err)
	}

	// Verify all models are registered
	registeredModels := registry.List()
	for i := 0; i < 10; i++ {
		name := "model-" + string(rune('a'+i))
		assert.Contains(t, registeredModels, name)

		retrieved, err := registry.Get(name)
		require.NoError(t, err)
		assert.Equal(t, models[i], retrieved)
	}
}

func TestEmbeddingModelRegistry_GetNonExistent(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	_, err := registry.Get("nonexistent-model")
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "not configured")
}

func TestEmbeddingModelRegistry_GetOrCreate_Concurrent(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	// Add a local config that can be created
	registry.configs["concurrent-test"] = EmbeddingModelConfig{
		Name:       "concurrent-test",
		Provider:   "local",
		Dimensions: 512,
	}

	var wg sync.WaitGroup
	results := make(chan EmbeddingModel, 100)
	errors := make(chan error, 100)

	// Concurrent GetOrCreate for the same model
	for i := 0; i < 100; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			model, err := registry.GetOrCreate("concurrent-test")
			if err != nil {
				errors <- err
			} else {
				results <- model
			}
		}()
	}

	wg.Wait()
	close(results)
	close(errors)

	// Check no errors
	for err := range errors {
		t.Errorf("Unexpected error: %v", err)
	}

	// All results should be the same model instance
	var firstModel EmbeddingModel
	for model := range results {
		if firstModel == nil {
			firstModel = model
		} else {
			assert.Equal(t, firstModel, model, "All concurrent calls should return the same model instance")
		}
	}
}

func TestEmbeddingModelRegistry_EncodeWithFallback_PartialFailures(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{
		FallbackChain: []string{"failing1", "failing2", "working"},
	})

	// Register failing models
	failing1 := NewMockEmbeddingModel("failing1", 768)
	failing1.SetEncodeError(errors.New("model 1 failure"))
	_ = registry.Register("failing1", failing1)

	failing2 := NewMockEmbeddingModel("failing2", 768)
	failing2.SetEncodeError(errors.New("model 2 failure"))
	_ = registry.Register("failing2", failing2)

	// Register working model
	working := NewMockEmbeddingModel("working", 768)
	working.embeddings = [][]float32{{1.0, 2.0, 3.0}}
	_ = registry.Register("working", working)

	// Test fallback
	embeddings, modelName, err := registry.EncodeWithFallback(context.Background(), []string{"test"})
	assert.NoError(t, err)
	assert.Equal(t, "working", modelName)
	assert.Len(t, embeddings, 1)
}

func TestEmbeddingModelRegistry_EncodeWithFallback_ConfiguredButNotCreated(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{
		FallbackChain: []string{"local-fallback"},
	})

	// local-fallback is configured by default but not created yet
	embeddings, modelName, err := registry.EncodeWithFallback(context.Background(), []string{"test"})
	assert.NoError(t, err)
	assert.Equal(t, "local-fallback", modelName)
	assert.Len(t, embeddings, 1)
}

func TestEmbeddingModelRegistry_Health_MixedResults(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	healthy1 := NewMockEmbeddingModel("healthy1", 768)
	healthy2 := NewMockEmbeddingModel("healthy2", 768)
	unhealthy1 := NewMockEmbeddingModel("unhealthy1", 768)
	unhealthy1.SetHealthError(errors.New("health check failed"))
	unhealthy2 := NewMockEmbeddingModel("unhealthy2", 768)
	unhealthy2.SetHealthError(errors.New("connection refused"))

	_ = registry.Register("healthy1", healthy1)
	_ = registry.Register("healthy2", healthy2)
	_ = registry.Register("unhealthy1", unhealthy1)
	_ = registry.Register("unhealthy2", unhealthy2)

	results := registry.Health(context.Background())

	assert.NoError(t, results["healthy1"])
	assert.NoError(t, results["healthy2"])
	assert.Error(t, results["unhealthy1"])
	assert.Error(t, results["unhealthy2"])
	assert.Len(t, results, 4)
}

func TestEmbeddingModelRegistry_Close_WithErrors(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	// Create a mock that returns an error on Close
	errorModel := &MockEmbeddingModelWithCloseError{
		MockEmbeddingModel: *NewMockEmbeddingModel("error-model", 768),
		closeError:         errors.New("close failed"),
	}

	_ = registry.Register("error-model", errorModel)
	_ = registry.Register("normal-model", NewMockEmbeddingModel("normal-model", 768))

	err := registry.Close()
	// Should return the last error
	assert.Error(t, err)

	// Models map should be cleared regardless
	assert.Empty(t, registry.models)
}

// MockEmbeddingModelWithCloseError is a mock that returns an error on Close
type MockEmbeddingModelWithCloseError struct {
	MockEmbeddingModel
	closeError error
}

func (m *MockEmbeddingModelWithCloseError) Close() error {
	return m.closeError
}

// =============================================================================
// OpenAI Embedding Model Extended Tests
// =============================================================================

func TestOpenAIEmbeddingModel_EncodeWithBatching(t *testing.T) {
	callCount := int32(0)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		atomic.AddInt32(&callCount, 1)

		var req map[string]interface{}
		_ = json.NewDecoder(r.Body).Decode(&req)

		inputs := req["input"].([]interface{})
		data := make([]map[string]interface{}, len(inputs))
		for i := range inputs {
			data[i] = map[string]interface{}{
				"embedding": make([]float32, 1536),
				"index":     i,
			}
		}

		response := map[string]interface{}{"data": data}
		_ = json.NewEncoder(w).Encode(response)
	}))
	defer server.Close()

	// Manually set the httpClient to use the test server
	config := EmbeddingModelConfig{
		Name:       "openai-test",
		Provider:   "openai",
		ModelID:    "text-embedding-3-small",
		Dimensions: 1536,
		APIKey:     "test-key",
		Timeout:    30 * time.Second,
	}

	model := NewOpenAIEmbeddingModel(config)
	// Override the HTTP client to use test server (need to modify request URL)
	// For this test, we verify the model structure is correct
	assert.Equal(t, "openai-test", model.Name())
	assert.Equal(t, 1536, model.Dimensions())
	assert.Equal(t, "openai", model.Provider())
}

func TestOpenAIEmbeddingModel_EncodeSingle(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		assert.Equal(t, "POST", r.Method)
		assert.Equal(t, "Bearer test-key", r.Header.Get("Authorization"))
		assert.Equal(t, "application/json", r.Header.Get("Content-Type"))

		response := map[string]interface{}{
			"data": []map[string]interface{}{
				{
					"embedding": []float32{0.1, 0.2, 0.3, 0.4, 0.5},
					"index":     0,
				},
			},
		}
		_ = json.NewEncoder(w).Encode(response)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:       "openai-test",
		Provider:   "openai",
		ModelID:    "text-embedding-3-small",
		Dimensions: 5,
		APIKey:     "test-key",
		Timeout:    30 * time.Second,
	}

	model := NewOpenAIEmbeddingModel(config)
	assert.NotNil(t, model)
}

func TestOpenAIEmbeddingModel_Encode_InvalidJSON(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte("invalid json{{{"))
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "openai-test",
		Timeout: 30 * time.Second,
	}

	model := NewOpenAIEmbeddingModel(config)
	assert.NotNil(t, model)
}

func TestOpenAIEmbeddingModel_Health_Error(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusInternalServerError)
		_, _ = w.Write([]byte(`{"error": "internal server error"}`))
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "openai-test",
		Timeout: 30 * time.Second,
	}

	model := NewOpenAIEmbeddingModel(config)
	assert.NotNil(t, model)
}

// =============================================================================
// Ollama Embedding Model Extended Tests
// =============================================================================

func TestOllamaEmbeddingModel_DefaultBaseURL_Extended(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:     "ollama-test",
		Provider: "ollama",
		ModelID:  "nomic-embed-text",
	}

	model := NewOllamaEmbeddingModel(config)
	assert.Equal(t, "http://localhost:11434", model.config.BaseURL)
}

func TestOllamaEmbeddingModel_CustomBaseURL(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:     "ollama-test",
		Provider: "ollama",
		ModelID:  "nomic-embed-text",
		BaseURL:  "http://custom-ollama:11434",
	}

	model := NewOllamaEmbeddingModel(config)
	assert.Equal(t, "http://custom-ollama:11434", model.config.BaseURL)
}

func TestOllamaEmbeddingModel_Encode_MultipleTexts(t *testing.T) {
	callCount := int32(0)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		atomic.AddInt32(&callCount, 1)
		response := map[string]interface{}{
			"embedding": []float32{0.1, 0.2, 0.3},
		}
		_ = json.NewEncoder(w).Encode(response)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:       "ollama-test",
		Provider:   "ollama",
		ModelID:    "nomic-embed-text",
		Dimensions: 3,
		BaseURL:    server.URL,
		Timeout:    30 * time.Second,
	}

	model := NewOllamaEmbeddingModel(config)
	embeddings, err := model.Encode(context.Background(), []string{"text1", "text2", "text3"})

	assert.NoError(t, err)
	assert.Len(t, embeddings, 3)
	// Ollama doesn't support batch, so it should call the API for each text
	assert.Equal(t, int32(3), atomic.LoadInt32(&callCount))
}

func TestOllamaEmbeddingModel_Encode_PartialFailure(t *testing.T) {
	callCount := int32(0)
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		count := atomic.AddInt32(&callCount, 1)
		if count == 2 {
			w.WriteHeader(http.StatusInternalServerError)
			_, _ = w.Write([]byte(`{"error": "failed"}`))
			return
		}
		response := map[string]interface{}{
			"embedding": []float32{0.1, 0.2, 0.3},
		}
		_ = json.NewEncoder(w).Encode(response)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:       "ollama-test",
		Provider:   "ollama",
		ModelID:    "nomic-embed-text",
		Dimensions: 3,
		BaseURL:    server.URL,
		Timeout:    30 * time.Second,
	}

	model := NewOllamaEmbeddingModel(config)
	_, err := model.Encode(context.Background(), []string{"text1", "text2", "text3"})

	assert.Error(t, err)
	assert.Contains(t, err.Error(), "failed to encode text 1")
}

// =============================================================================
// Sentence Transformers Model Extended Tests
// =============================================================================

func TestSentenceTransformersModel_Health_Success(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if r.URL.Path == "/health" {
			w.WriteHeader(http.StatusOK)
			return
		}
		t.Errorf("Unexpected path: %s", r.URL.Path)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "st-test",
		BaseURL: server.URL,
		Timeout: 30 * time.Second,
	}

	model := NewSentenceTransformersModel(config)
	err := model.Health(context.Background())
	assert.NoError(t, err)
}

func TestSentenceTransformersModel_Health_Failure(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusServiceUnavailable)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "st-test",
		BaseURL: server.URL,
		Timeout: 30 * time.Second,
	}

	model := NewSentenceTransformersModel(config)
	err := model.Health(context.Background())
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "503")
}

func TestSentenceTransformersModel_Encode_Success(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		assert.Equal(t, "/encode", r.URL.Path)

		var req map[string]interface{}
		_ = json.NewDecoder(r.Body).Decode(&req)

		texts := req["texts"].([]interface{})
		embeddings := make([][]float32, len(texts))
		for i := range texts {
			embeddings[i] = []float32{0.1, 0.2, 0.3}
		}

		response := map[string]interface{}{
			"embeddings": embeddings,
		}
		_ = json.NewEncoder(w).Encode(response)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:       "st-test",
		ModelID:    "all-mpnet-base-v2",
		Dimensions: 3,
		BaseURL:    server.URL,
		Timeout:    30 * time.Second,
	}

	model := NewSentenceTransformersModel(config)
	embeddings, err := model.Encode(context.Background(), []string{"text1", "text2"})

	assert.NoError(t, err)
	assert.Len(t, embeddings, 2)
}

func TestSentenceTransformersModel_Encode_InvalidResponse(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte("invalid json"))
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "st-test",
		BaseURL: server.URL,
		Timeout: 30 * time.Second,
	}

	model := NewSentenceTransformersModel(config)
	_, err := model.Encode(context.Background(), []string{"text"})

	assert.Error(t, err)
	assert.Contains(t, err.Error(), "decode")
}

// =============================================================================
// Local Hash Model Extended Tests
// =============================================================================

func TestLocalHashModel_GenerateHashEmbedding_Consistency(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "local-test",
		Provider:   "local",
		Dimensions: 256,
	}

	model := NewLocalHashModel(config)

	// Same input should produce same output
	embedding1 := model.generateHashEmbedding("test input")
	embedding2 := model.generateHashEmbedding("test input")
	assert.Equal(t, embedding1, embedding2)

	// Different input should produce different output
	embedding3 := model.generateHashEmbedding("different input")
	assert.NotEqual(t, embedding1, embedding3)
}

func TestLocalHashModel_GenerateHashEmbedding_Normalization(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "local-test",
		Provider:   "local",
		Dimensions: 512,
	}

	model := NewLocalHashModel(config)
	embedding := model.generateHashEmbedding("test input")

	// Calculate L2 norm
	var sum float32
	for _, v := range embedding {
		sum += v * v
	}
	norm := float32(1.0)
	if sum > 0 {
		norm = sum
	}

	// Norm should be approximately 1.0 (unit vector)
	assert.InDelta(t, 1.0, float64(norm), 0.001)
}

func TestLocalHashModel_Encode_EmptyInput(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "local-test",
		Provider:   "local",
		Dimensions: 128,
	}

	model := NewLocalHashModel(config)

	embeddings, err := model.Encode(context.Background(), []string{})
	assert.NoError(t, err)
	assert.Len(t, embeddings, 0)
}

func TestLocalHashModel_Encode_LargeInput(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "local-test",
		Provider:   "local",
		Dimensions: 1536,
		MaxTokens:  100000,
	}

	model := NewLocalHashModel(config)

	// Generate a large batch
	texts := make([]string, 1000)
	for i := range texts {
		texts[i] = "text " + string(rune('a'+i%26))
	}

	embeddings, err := model.Encode(context.Background(), texts)
	assert.NoError(t, err)
	assert.Len(t, embeddings, 1000)

	for _, emb := range embeddings {
		assert.Len(t, emb, 1536)
	}
}

func TestLocalHashModel_Health_AlwaysHealthy(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:     "local-test",
		Provider: "local",
	}

	model := NewLocalHashModel(config)

	// Health should always return nil
	err := model.Health(context.Background())
	assert.NoError(t, err)

	// Even with cancelled context
	ctx, cancel := context.WithCancel(context.Background())
	cancel()
	err = model.Health(ctx)
	assert.NoError(t, err)
}

// =============================================================================
// Registry createModel Tests
// =============================================================================

func TestEmbeddingModelRegistry_createModel_AllProviders(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	tests := []struct {
		name         string
		config       EmbeddingModelConfig
		expectError  bool
		providerName string
	}{
		{
			name: "openai provider",
			config: EmbeddingModelConfig{
				Name:       "openai-test",
				Provider:   "openai",
				Dimensions: 1536,
				APIKey:     "test-key",
			},
			expectError:  false,
			providerName: "openai",
		},
		{
			name: "ollama provider",
			config: EmbeddingModelConfig{
				Name:       "ollama-test",
				Provider:   "ollama",
				Dimensions: 768,
				BaseURL:    "http://localhost:11434",
			},
			expectError:  false,
			providerName: "ollama",
		},
		{
			name: "sentence-transformers provider",
			config: EmbeddingModelConfig{
				Name:       "st-test",
				Provider:   "sentence-transformers",
				Dimensions: 768,
				BaseURL:    "http://localhost:8080",
			},
			expectError:  false,
			providerName: "sentence-transformers",
		},
		{
			name: "local provider",
			config: EmbeddingModelConfig{
				Name:       "local-test",
				Provider:   "local",
				Dimensions: 512,
			},
			expectError:  false,
			providerName: "local",
		},
		{
			name: "unknown provider",
			config: EmbeddingModelConfig{
				Name:     "unknown-test",
				Provider: "unknown-provider",
			},
			expectError: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			model, err := registry.createModel(tt.config)

			if tt.expectError {
				assert.Error(t, err)
				assert.Nil(t, model)
				assert.Contains(t, err.Error(), "unknown embedding provider")
			} else {
				assert.NoError(t, err)
				assert.NotNil(t, model)
				assert.Equal(t, tt.providerName, model.Provider())
			}
		})
	}
}

// =============================================================================
// Context Cancellation Tests
// =============================================================================

func TestOpenAIEmbeddingModel_ContextCancellation(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		time.Sleep(5 * time.Second) // Simulate slow response
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "openai-test",
		Timeout: 100 * time.Millisecond,
	}

	model := NewOpenAIEmbeddingModel(config)

	// This should fail due to context cancellation
	// Note: The actual request would need to go to a slow server
	assert.NotNil(t, model)
}

func TestOllamaEmbeddingModel_ContextCancellation(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		time.Sleep(5 * time.Second)
	}))
	defer server.Close()

	config := EmbeddingModelConfig{
		Name:    "ollama-test",
		BaseURL: server.URL,
		Timeout: 100 * time.Millisecond,
	}

	model := NewOllamaEmbeddingModel(config)

	ctx, cancel := context.WithTimeout(context.Background(), 50*time.Millisecond)
	defer cancel()

	_, err := model.EncodeSingle(ctx, "test")
	assert.Error(t, err)
}

// =============================================================================
// Mock Cache Tests
// =============================================================================

func TestMockEmbeddingCache_Concurrent(t *testing.T) {
	cache := NewMockEmbeddingCache()

	var wg sync.WaitGroup
	for i := 0; i < 100; i++ {
		wg.Add(2)

		go func(idx int) {
			defer wg.Done()
			cache.Set("key-"+string(rune('a'+idx%26)), []float32{float32(idx)}, time.Hour)
		}(i)

		go func(idx int) {
			defer wg.Done()
			cache.Get("key-" + string(rune('a'+idx%26)))
		}(i)
	}

	wg.Wait()
}

func TestMockEmbeddingCache_Delete(t *testing.T) {
	cache := NewMockEmbeddingCache()

	cache.Set("key1", []float32{1.0, 2.0}, time.Hour)
	val, ok := cache.Get("key1")
	assert.True(t, ok)
	assert.Equal(t, []float32{1.0, 2.0}, val)

	cache.Delete("key1")
	val, ok = cache.Get("key1")
	assert.False(t, ok)
	assert.Nil(t, val)
}

// =============================================================================
// Interface Compliance Tests
// =============================================================================

func TestOpenAIEmbeddingModel_ImplementsEmbeddingModel(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "test",
		Dimensions: 1536,
	}
	model := NewOpenAIEmbeddingModel(config)

	// Verify interface compliance
	var _ EmbeddingModel = model
}

func TestOllamaEmbeddingModel_ImplementsEmbeddingModel(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "test",
		Dimensions: 768,
	}
	model := NewOllamaEmbeddingModel(config)

	var _ EmbeddingModel = model
}

func TestSentenceTransformersModel_ImplementsEmbeddingModel(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "test",
		Dimensions: 768,
	}
	model := NewSentenceTransformersModel(config)

	var _ EmbeddingModel = model
}

func TestLocalHashModel_ImplementsEmbeddingModel(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "test",
		Dimensions: 512,
	}
	model := NewLocalHashModel(config)

	var _ EmbeddingModel = model
}

// =============================================================================
// Edge Cases Tests
// =============================================================================

func TestLocalHashModel_ZeroDimensions(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:       "test",
		Provider:   "local",
		Dimensions: 0, // Zero dimensions
	}

	model := NewLocalHashModel(config)
	// Should default to 1536
	assert.Equal(t, 1536, model.Dimensions())
}

func TestOpenAIEmbeddingModel_EmptyAPIKey(t *testing.T) {
	config := EmbeddingModelConfig{
		Name:   "test",
		APIKey: "", // Empty API key
	}

	model := NewOpenAIEmbeddingModel(config)
	assert.NotNil(t, model)
	// Model should still be created, but API calls would fail
}

func TestEmbeddingModelRegistry_EncodeSingleWithFallback_Extended(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{
		FallbackChain: []string{"local-fallback"},
	})

	embedding, modelName, err := registry.EncodeSingleWithFallback(context.Background(), "single text")
	assert.NoError(t, err)
	assert.Equal(t, "local-fallback", modelName)
	assert.NotEmpty(t, embedding)
	assert.Len(t, embedding, 1536) // Default local dimensions
}

func TestEmbeddingModelRegistry_EncodeSingleWithFallback_AllFail(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{
		FallbackChain: []string{},
	})
	// Clear fallback chain
	registry.fallbackChain = []string{}

	_, _, err := registry.EncodeSingleWithFallback(context.Background(), "test")
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "all embedding models in fallback chain failed")
}

// =============================================================================
// Logger Tests
// =============================================================================

func TestEmbeddingModelRegistry_CustomLogger(t *testing.T) {
	logger := logrus.New()
	logger.SetLevel(logrus.DebugLevel)

	registry := NewEmbeddingModelRegistry(RegistryConfig{
		Logger: logger,
	})

	assert.Equal(t, logger, registry.logger)
}

func TestEmbeddingModelRegistry_DefaultLogger(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{})

	assert.NotNil(t, registry.logger)
}

// =============================================================================
// Config Override Tests
// =============================================================================

func TestEmbeddingModelRegistry_ConfigOverride(t *testing.T) {
	registry := NewEmbeddingModelRegistry(RegistryConfig{
		Configs: map[string]EmbeddingModelConfig{
			"custom-model": {
				Name:       "custom-model",
				Provider:   "local",
				Dimensions: 256,
				MaxTokens:  1000,
			},
		},
	})

	// Verify custom config was added
	config, exists := registry.configs["custom-model"]
	assert.True(t, exists)
	assert.Equal(t, "custom-model", config.Name)
	assert.Equal(t, 256, config.Dimensions)

	// Verify default local-fallback still exists
	_, exists = registry.configs["local-fallback"]
	assert.True(t, exists)
}

// =============================================================================
// HTTP Error Response Tests
// =============================================================================

func TestOpenAIEmbeddingModel_HTTPErrors(t *testing.T) {
	tests := []struct {
		name       string
		statusCode int
		body       string
	}{
		{"bad request", http.StatusBadRequest, `{"error": "invalid request"}`},
		{"unauthorized", http.StatusUnauthorized, `{"error": "invalid api key"}`},
		{"forbidden", http.StatusForbidden, `{"error": "access denied"}`},
		{"not found", http.StatusNotFound, `{"error": "model not found"}`},
		{"rate limit", http.StatusTooManyRequests, `{"error": "rate limit exceeded"}`},
		{"internal error", http.StatusInternalServerError, `{"error": "internal server error"}`},
		{"service unavailable", http.StatusServiceUnavailable, `{"error": "service unavailable"}`},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
				w.WriteHeader(tt.statusCode)
				_, _ = w.Write([]byte(tt.body))
			}))
			defer server.Close()

			// Model creation should succeed
			config := EmbeddingModelConfig{
				Name:    "openai-test",
				APIKey:  "test-key",
				Timeout: 30 * time.Second,
			}
			model := NewOpenAIEmbeddingModel(config)
			assert.NotNil(t, model)
		})
	}
}

// =============================================================================
// Stress Tests
// =============================================================================

func TestLocalHashModel_StressTest(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping stress test in short mode")
	}

	config := EmbeddingModelConfig{
		Name:       "stress-test",
		Provider:   "local",
		Dimensions: 1536,
	}

	model := NewLocalHashModel(config)

	// Generate many embeddings concurrently
	var wg sync.WaitGroup
	errors := make(chan error, 1000)

	for i := 0; i < 1000; i++ {
		wg.Add(1)
		go func(idx int) {
			defer wg.Done()
			text := "test text " + string(rune('a'+idx%26))
			_, err := model.EncodeSingle(context.Background(), text)
			if err != nil {
				errors <- err
			}
		}(i)
	}

	wg.Wait()
	close(errors)

	for err := range errors {
		t.Errorf("Unexpected error: %v", err)
	}
}

func TestEmbeddingModelRegistry_StressTest(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping stress test in short mode")
	}

	registry := NewEmbeddingModelRegistry(RegistryConfig{
		FallbackChain: []string{"local-fallback"},
	})

	var wg sync.WaitGroup
	errors := make(chan error, 500)

	// Concurrent encoding with fallback
	for i := 0; i < 500; i++ {
		wg.Add(1)
		go func(idx int) {
			defer wg.Done()
			text := "test text " + string(rune('a'+idx%26))
			_, _, err := registry.EncodeWithFallback(context.Background(), []string{text})
			if err != nil {
				errors <- err
			}
		}(i)
	}

	wg.Wait()
	close(errors)

	for err := range errors {
		t.Errorf("Unexpected error: %v", err)
	}
}
